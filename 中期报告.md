# 高阶挑战项目(中期报告)  

# 基于多智能体强化学习的无人机集群任务分配问题研究  

学院名称 未来空天技术学院专业名称 未来空天领军计划学生姓名 李中杨行业导师 张瑞雨校内导师 吕金虎  

# 目录  

1 选题情况.  

1.1 研究背景  
1.2 研究意义  
1.3 国内外研究现状. 3  

1.3.1 多机器人任务分配分类.1.3.2 传统任务分配算法研究现状1.3.3 基于深度强化学习的任务分配算法研究现状  
2 研究方案 62.1 研究目标. 62.2 研究内容 6  
3 研究进展. 73.1 总体研究进展.3.2 主要研究成果.3.2.1 多智能体粒子环境搭建. 73.2.2 启发式算法初步解决任务分配 93.2.3 设计深度强化学习神经网络解决任务分配问题 . 103.2.4 用神经网络解决异构无人机集群任务分配问题 163.2.5 用神经网络解决任务数量变化的动态任务分配问题 17  
4 尚存问题及后期工作安排 184.1 尚存问题 184.1.1 动态环境适应问题 184.1.2 算法对比与实机验证. 18  

4.2 后期工作安排 18  
参考文献. 19  

# 无人机集群任务分配智能方法研究  

学 生：李中杨行业导师：张瑞雨校内导师：吕金虎  

# 1 选题情况  

# 1.1 研究背景  

随着电子信息技术、自动化技术及通信技术的快速发展，以无人机系统为代表的无人系统和技术得到了广泛的研究和应用。集群化是无人机行业发展的必然趋势，相比于单架无人机，集群化操作的无人机系统展现了显著的优势，它通过多架无人机协同工作，能够缩短操作时间、降低任务失败率，并具有多任务处理能力[1]。  

![](images/c7995d68457ed95ebc1a34ab1737ecbad38b92495e94c025882e2b2a37635cea.jpg)  
图 1.1 无人机集群应用场景  

任务分配技术作为无人机协同控制的顶层规划，需要根据各种信息检测系统和相关设备提供的环境信息，结合无人机自身的性能参数，将状态参数特定的任务分配给多架无人机执行，并可以随着动态环境变化对任务计划进行实时协商和调整。一个有效的任务分配方案可以充分利用各无人机的性能，为提高任务完成效率提供强有力的保证[2]。传统的任务分配方法主要有基于拍卖的方法、基于博弈论的方法与基于优化的方法，上述方法在应对静态任务分配或小规模集群任务时表现良好，但在处理大规模动态任务场景时，因计算复杂度高或对任务需求变化反应迟缓而表现出明显不足[3]。  

近年来，随着人工智能的发展，人工智能技术为无人机集群的任务分配提供了新的思路。其中，多智能体深度强化学习作为一种数据驱动的优化方法，能够通过训练形成“肌肉记忆”，使无人机在任务需求动态变化的环境中实现实时响应与协同调整。相比于传统方法，深度强化学习不仅能够减少计算时间，还能有效提升动态适应性[3]。  

![](images/9b74101190c8002cbc9ef566533dc761ec781e3097e0acdf563035b1d1645713.jpg)  
图 1.2 多智能体强化学习建模  

此外，为了优化无人机的协作效率，研究者进一步提出了动态联盟构建与基于注意力机制的深度强化学习模型。通过在任务选择过程中动态形成或解散联盟，无人机能够依据任务需求灵活调整策略，减少在分配和执行过程中的等待时间与路径成本[3]。这些技术的引入，使得大规模任务分配中的序贯决策问题得到了显著优化，为大规模无人机集群在复杂场景中的任务分配提供了更加灵活和高效的解决方案。  

但目前基于深度强化学习的相关研究普遍存在收敛速度慢、训练阶段计算代价高、集成难度大等问题，需要进一步改进。本研究通过增添任务完成率奖励，创新输入、网络与训练机制，有望解决现有研究存在的问题，并将任务分配扩展到异构与动态领域。  

# 1.2 研究意义  

本研究结合深度强化学习和智能仿生技术，旨在解决大规模无人机集群的序贯任务分配问题。通过引入基于 Transformer 的神经网络架构与动态奖励机制，探索高效的分布式任务分配策略，最终在多任务、高动态的复杂场景中提升无人机集群的任务完成率、实时性与适应性，为无人机集群的实际部署提供重要的理论与技术支撑。  

相较以往的方法，本研究提出的序贯任务分配智能决策方法主要有以下两点优势：  

1.灵活性和适应性增强，本研究方法使无人机能够基于实时数据做出决策，并适应不断变化的环境，从而在动态和不确定的环境中表现出更高的灵活性和适应性[1]；2.在得到近似最优解的同时计算时间减少，初步测试结果表明[3]，本研究方法能够与基于混合整数规划（Mixed Integer Programming,MIP）的精确求解器相媲美或超越，同时将计算时间至少减少两个数量级。这使得本研究方法在多个领域都具有重要的应用价值。例如，在灾难响应和救援操作中，这项技术可以使无人机集群快速评估受影响区域的情况，动态分配任务以最有效地搜索和提供援助。此外，这种技术也可以应用于军事侦察打击、农业监测、交通管理和环境监测等领域，并取得比传统方法更优异的表现。  

# 1.3 国内外研究现状  

# 1.3.1 多机器人任务分配分类  

由于对多机器人系统（如无人机集群）的日益关注，多机器人协调受到了广泛的关注。多机器人任务分配近年来日益突出，并已成为一个重要的研究课题。任务分配是指在多机器人中，通过决策，将任务分配给各个机器人执行，以满足任务相关约束和要求的过程。为了解决多机器人任务分配问题，首先需要对其进行分析与分类。目前多机器人任务分配主要从以下三个层面进行分类[4]：  

表 1.1 任务分配分类  


<html><body><table><tr><td>分类维度</td><td colspan="2">类别</td></tr><tr><td>机器人能力</td><td>单任务机器人 (ST)</td><td>多任务机器人 (MT)</td></tr><tr><td>任务要求</td><td>单机器人任务 (SR)</td><td>多机器人任务 (MR)</td></tr><tr><td>分配方式</td><td>即时分配 (IA)</td><td>长期分配（TA)</td></tr></table></body></html>  

单任务机器人（ST）和多任务机器人（MT）：ST 表示每个机器人一次最多能够执行一个任务，而MT 表示一些机器人可以同时执行多个任务。  

单机器人任务（SR）和多机器人任务（MR）：SR 意味着每个任务只需要一个任务，MR 意味着有些任务可能需要多个机器人。  

即时分配（IA）和长期分配（TA）：IA 是指有关机器人、任务和环境的可用信息只允许将任务即时分配给机器人，而不需要为未来的分配做计划；TA 意味着有更多的信息，比如需要分配的所有任务集，或者随着时间的推移任务如何到达的模型。  

本研究的智能决策方法专注于解决单任务无人机(ST)－多无人机任务(MR)的长期分配(TA)问题，以实现无人机集群面对大量不同需求序贯任务时的快速响应和最优分配。  

# 1.3.2 传统任务分配算法研究现状  

![](images/7a1f55ffc981a05c032e53c4c4b836b45aa28a2138ad6e57ec311c7edd12b7f0.jpg)  
图 1.3 传统任务分配算法  

任务分配技术是一种优化技术，将多个任务分配给多个代理，以最小化给定的目标函数。任务分配在许多基于多智能体的应用中非常重要，因为它确保所有任务都在一定的约束下正确执行。在任务分配过程中，需要高水平的协作，以便无人机能够快速交流信息、协调任务，并使用适当的任务调度和管理技术将任务分配给其他无人机。协作和信息交换的高要求，使独立任务分配的过程变得复杂。传统的任务分配技术，如基于拍卖的、基于博弈论的与基于优化的方法，依赖于某些预先定义的规则和决策标准。这些算法依赖于对环境的过往知识，通常旨在提高各种性能指标，如任务完成时间、无人机能耗和资源分配[1]。  

基于拍卖的技术是处理动态环境中任务分配问题的一种有效方法。该技术的思想是模拟某种拍卖程序，并根据投标结果将任务分配给无人机，其中任务被分配给出价最高的无人机。文章[5]提出了一种“两阶段”拍卖技术，用于多无人机动态任务分配和障碍物路径规划。文章[6]中提到的基于共识的拍卖算法（CBAA）是一种单任务分配技术，允许无人机对任务进行投标，并提供了一种分散共识的方法，在收敛性和性能方面优于以往的拍卖算法。拍卖算法具有规则和操作简单的优点，但该方法的计算成本高，所以不适合大规模无人机集群。  

基于博弈论的方法将任务分配问题视为一个博弈，每个无人机都是博弈的参与者。通过利用纳什议价博弈，该方法将多任务分配映射为多个博弈方对多个任务的博弈模型，并采用某种方法（如空间距离的方法）计算多个博弈用户对多个任务的纳什议价解，最终实现多个任务在多个参与用户之间的有效分配。文章[7]提出了一种基于博弈的任务分配方法，该方法鼓励传感器形成联盟以竞标无人机数据收集服务，并设计了一种并行变量邻域上升搜索算法，以快速搜索近似最优的团购联盟结构。基于博弈论的方法通过博弈论的分析，找到博弈均衡点，能使分配方案更稳定，但该方法也存在计算复杂度高与策略求解难度高的问题。  

基于优化的方法主要通过数学模型来寻找最优或近似最优的任务分配方案。对这类方案的求解主要有两种思路，一种是精确搜索，即对问题所有可能的解空间进行遍历搜索，典型的有整数规划算法[8]，这种思路能得到全局最优解，但需要很大的计算资源，并且计算时间较长；另一种是启发式搜索，在搜索过程中加入一定启发因子，指导搜索向一个比较小的范围内进行，如模拟退火、禁忌搜索和遗传算法等等。该思路能提供高质量的解决方案，并能处理具有复杂约束和限制的任务分配问题，但有时只能局部最优解，并且实时性较低，不能很好地处理高度动态的环境。此外，基于优化的方法一般基于全局观察，是一种集中式的方法，难以满足动态和不稳定环境的需求。  

综上所述，传统方法的主要问题在于计算成本和复杂度较高，实时性低，无法适应具有高度不确定度的环境，因此需要一种新的方法来解决这些问题。  

# 1.3.3 基于深度强化学习的任务分配算法研究现状  

大多数现实世界的任务分配场景都涉及高度动态环境，其变化情况无法描述。仅仅通过明确的数学模型，预测未来无人机在这种环境中可能遇到的干扰极具挑战性。深度强化学习通过考虑无人机以前的经验，为无人机的行动提供训练与指导，其主要模式是将环境与目标抽象为马尔可夫决策，通过设定一定的奖励函数，使无人机能够适应高度动态的环境[9]。文章[10]提出了一种基于深度强化学习的双向请求－响应算法，通过将请求无人机和响应无人机的决策定义为马尔卡夫决策过程的相关元素，将动态任务分配问题转换为了学习最优请求和响应策略。文章[3]提出了一种基于深度强化学习的任务分配算法，针对序贯任务分配问题，每个机器人动态地构建任务分配和路径选择策略。算法采用注意力机制的神经网络框架，结合“领导－跟随”策略简化决策过程，提高了模型的训练效率和任务分配质量。实验表明，该方法在复杂动态场景中较传统优化方法能显著提升计算效率，同时在任务分配质量上接近甚至优于基于混合整数规划的精确求解方法。但总体来说，基于深度强化学习的方法的相关研究较少，且还存在收敛速度慢、训练阶段计算代价高、集成难度大等问题亟待解决[1]，需要进一步的研究与试验。  

# 2 研究方案  

# 2.1 研究目标  

针对传统无人机集群任务分配方法在大规模、动态任务环境下实时性、灵活性差，难以应用于实际无人机集群决策的现状，本项目旨在通过将深度学习、多智能体强化学习等人工智能方法引入无人机集群序贯任务分配问题，研究解决大规模无人机集群自主决策问题。进一步提升无人机集群系统自主性。在大规模、高动态的复杂决策场景中，习得超越专业集群指控人员的任务决策能力。  

(1)首先验证深度强化学习方法在大规模任务分配中的可行性。(2)对比基准算法（如基于启发式优化或博弈论的传统方法）和深度强化学习算法在任务完成率、资源利用率、决策时间等方面的表现。(3)探索大规模动态任务环境下算法的适应能力和稳定性。  

# 2.2 研究内容  

1.研究现状调研及集群智能决策推演训练平台开发  

对无人机集群任务分配领域的国内外文献进行系统调研，总结传统方法的优缺点，分析当前在动态环境下任务分配所面临的主要技术瓶颈，为后续算法设计提供理论依据。  

构建多智能体交互仿真环境，实现对任务、无人机（智能体）和任务场景（地标）的高度自定义，支持实时渲染、暂停、任务动态添加和环境参数调节，确保实验平台能够真实模拟实际动态任务分配场景。  

2.数学建模与深度强化学习网络构建：  

将无人机集群任务分配抽象为序贯决策问题，构建包含任务、无人机、时空约束的多维模型，设计目标函数和约束条件，为深度强化学习算法提供数学基础。  

针对任务分配的动态性，设计基于 Attention 机制的神经网络，引入Transformer 架构编码解码无人机与任务信息。网络结构包括输入嵌入层、编码器、解码器及多头注意力机制，能捕捉任务关联性和无人机协同策略，输出任务选择概率。  

3.异构无人机任务分配与动态环境扩展：  

研究扩展至异构无人机集群的任务分配问题，考虑无人机性能差异，设置不同参数和约束，实现协同分配，适应多样化任务。  

开发基于深度强化学习的自适应任务分配策略，使无人机集群能实时调整任务分配，应对动态环境变化，并通过奖励机制优化分配效果。  

# 3 研究进展  

# 3.1 总体研究进展  

![](images/a298c88fe773b9b69b131e74a8f578ce79107222384f218f1a8e1d54d1b82f8e.jpg)  
图 3.1 总体研究进展  

# 3.2 主要研究成果  

# 3.2.1 多智能体粒子环境搭建  

目前针对无人机集群的任务分配问题，大部分研究仅对其进行了数学建模，最终用matplotlib 生成结果演示图。但matplotlib 生成的结果演示图无法随代码运行实时且动态地演示任务分配进程，并在进程中实时修改任务或无人机的数量，以验证模型的适应性和灵活性。  

![](images/803fe54944a04c74816810c608e3018ecfc5159ec121ddc619a51561d5b1d5dd.jpg)  
图 3.2 任务分配问题研究结果演示图[11]  

为了使用深度强化学习解决任务分配问题，并进行可视化演示，本研究基于多智能体粒子环境框架[12]，重新设计了一套适用于无人机任务分配等任务的实时模拟系统。该系统能够实时反馈集群中各无人机的状态及任务执行情况，通过强化学习算法的自我迭代，最终训练出实现高效的任务分配策略。此外，该场景系统还具备扩展性，可容纳不同种类和数量的无人机，并能在运行过程中实时修改无人机和任务数量，以适应动态变化的真实环境：  

![](images/469dd9f0feedd1aa6ce6fde4d05a89e4c092fc889aaf7e36d0e692f461e2b427.jpg)  
图 3.3 场景演示  

环境中各个元素的定义如下：  

1.智能体：在环境中通过感知、行动协同完成任务的自主智能体，其行为由端到端训练的神经网络策略通过强化学习获得，如无人机。  

2.任务：物理环境中具有颜色、形状等特征的静态目标，用作智能体在行动中定位和参考的参照物，如任务或仓库，在本研究任务分配场景中，蓝色表示未完成任务，其中心数字表示当前任务需求，绿色表示已完成任务。  

# 3.2.2 启发式算法初步解决任务分配  

OR-Tools 是 Google 推出的一个开源组合优化工具库，旨在帮助开发者从海量可行方案中快速寻找最优或近似最优解，其应用领域涵盖线性规划、混合整数规划、约束编程、网络流以及车辆路径规划（VRP）等问题。本研究的单任务无人机（ST）与多无人机任务（MR）间的长期分配（TA）问题，可分解为多个需求不同的车辆路径规划问题。针对该问题，OR-Tools 通过将实际任务分配网络抽象为一个图模型来实现求解：每个节点代表一个任务或仓库，每条边则表示两节点间的距离、行驶时间或运输成本。该工具库提供了专门的 VRP 模块，其中的 RoutingIndexManager 用于在内部管理实际节点与求解器内部索引的映射，而 RoutingModel 则构建整个调度问题模型，通过预设启发式策略（例如最便宜弧策略）生成初始解，再利用局部搜索、禁忌搜索、引导局部搜索等高级优化技术对解进行迭代改进，最终在满足所有约束条件的前提下，实现总完成时间的最小化。  

![](images/0b3a092832792a03641335ee000007a623537174b2c1f92df56f5968647c0c8f.jpg)  
图 3.4 OR-Tools 解决任务分配问题  

# 3.2.3 设计深度强化学习神经网络解决任务分配问题  

A. 任务分配问题强化学习建模[13]无人机集合为 $A = \{ a _ { 1 } , a _ { 2 } , . . . a _ { n } \}$ ，每个无人机具备特定的任务完成能力 $C _ { a _ { j } } = [ c _ { 1 } ^ { j } , c _ { 2 } ^ { j } , . . . , c _ { l } ^ { j } ]$ 。场景中的任务集合为 $M = \{ m _ { 1 } , m _ { 2 } , . . . , m _ { { m } _ { m } } \}$ ，每个任务具有特定的能力需求  
$\boldsymbol { q } _ { m _ { i } } = [ \boldsymbol { q } _ { 1 } ^ { j } , \boldsymbol { q } _ { 2 } ^ { j } , . . . , \boldsymbol { q } _ { l } ^ { j } ]$ ，还具有任务完成时间需求 $t _ { { { m } _ { i } } }$ 。  

在运行过程中，智能体在每个时刻有观察 $O _ { i } ^ { t } = \{ T _ { i } ^ { t } , N _ { i } ^ { t } , M _ { i } ^ { t } \}$ ，其中：  

$T _ { i } ^ { t } = \left[ \begin{array} { c c c c c c } { { - t } } & { { } } & { { } } & { { } } & { { } } \\ { { m _ { i } , } } & { { m _ { i } , } } & { { m _ { i } - } } & { { _ { a _ { i } } , y _ { m _ { i } } - } } & { { _ { a _ { i } } , _ { m _ { i } } , } } & { { _ { m _ { i } } , } } \end{array} \right]$ 表示对于任务的观察， $\overline { { q } } _ { m _ { i } } ^ { t } = q _ { m _ { i } } - c _ { L } ^ { t }$ 表示剩余的能力需求， $x _ { m _ { i } } y _ { m _ { i } } , x _ { a _ { i } } y _ { a _ { i } }$ 分别为任务和智能体的坐标， $d _ { { \scriptscriptstyle m } }$ 是智能体到达该任务耗时， $f$ 表示任务是否完成。  

$N _ { t i } = \left[ \mathbf { c } _ { a _ { j } } , \mathbf { d } , x _ { a _ { j } } - x _ { a _ { i } } , y _ { a _ { j } } - y _ { a _ { i } } , e _ { j } \right]$ 表示所有智能体工作状态， $\mathbf { d } \in R ^ { 1 \times 3 }$ 为该智能体任务剩余执行时间、剩余旅行时间、等待时间， $e _ { j } \in \{ 0 , 1 \}$ 表示智能体是自由的（0）或在进程中（1）。  

$\boldsymbol { M } _ { i } ^ { t }$ 是二元掩码，表示任务是否对该智能体开放。  

在训练过程中，每次执行结束，环境都会生成一个奖励，表示本次执行效果的好坏，用于后续网络的训练，奖励的表示为 $R ( \Phi ) = - T - W$ 其中 $\intercal$ 表示全体执行时间，W 表示执行效率，具体表达式为：  

$$
W = \frac { 1 } { k _ { m } } \sum _ { i = 1 } ^ { k _ { n } } w _ { i } ,
$$  

$$
w _ { i } = \left\{ \begin{array} { l l } { \displaystyle \sum _ { j = 1 } ^ { k _ { b } } \Big | c _ { L } ^ { ( j ) } - q _ { m _ { i } } ^ { ( j ) } \Big | } & \\ { \displaystyle \sum _ { j = 1 } ^ { k _ { b } } q _ { m _ { i } } ^ { ( j ) } } & { \mathrm { i f ~ } c _ { L } \succeq q _ { m _ { i } } } \\ { \eta , } & { \mathrm { i f ~ } \oiiint } \end{array} \right.
$$  

# B. Attention 机制[14]  

Attention 层如今深度学习领域应用最广泛的网络机制，其在文本生成、图像处理等领域都有优越的表现。Attention 层的运行机制主要体现在它如何根据输入中的关键信息自动调整“注意力”，使得模型在生成输出时能够聚焦于最相关的部分。  

# 一、输入映射：生成 Q、K、V 向量  

将各种形式的输入转化为特定形式的嵌入矩阵 X（例如，每个词的嵌入向量），通过三个独立的全连接层分别生成查询（Query，Q）、键（Key，K）和值（Value，V）：  

$$
Q = X W ^ { \varrho } , K = X W ^ { \kappa } , V = X W ^ { V }
$$  

其中 W 是可学习的权重矩阵。这一步的目的是将原始输入映射到不同的子空间，便于后续比较和聚合信息。  

# 二、计算相似度得分  

对于每个查询向量 $q _ { i }$ 和所有键向量 $k _ { j }$ ，计算它们的点积来衡量相似性：  

$$
s _ { i j } = q _ { i } \cdot k _ { j } ^ { T }
$$  

为了防止随着向量维度增大，点积值变得过大而导致 softmax 梯度过小，通常将得分除以 $\sqrt { d _ { k } }$ ​ （其中 $d _ { k }$ ​ 是键向量的维度）：  

$$
\hat { s } _ { i j } = \frac { s _ { i j } } { \sqrt { d _ { k } } }
$$  

这一操作有助于训练的稳定性。  

![](images/61e26d22f27742489101a94cdc632a94872c9e26594518e8fd6619e2c25439f5.jpg)  
图 3.5 输入映射与计算相似度得分  

三、归一化：计算注意力权重  

对每个查询 $q _ { i }$ 的得分向量 $\hat { s } _ { i } = [ \hat { s } _ { i 1 } , \hat { s } _ { i 2 } , . . . , \hat { s } _ { i n } ]$ 施加 softmax 操作，将其转化为概率分布：  

$$
\alpha _ { i j } = \frac { \exp ( \hat { s } _ { i j } ) } { \displaystyle \sum _ { j = 1 } ^ { n } \exp ( \hat { s } _ { i j } ) }
$$  

这里 $\alpha _ { i j }$ 就表示查询 $q _ { i }$ 对键 $k _ { j }$ 的“关注程度”或注意力权重。  

四、聚合信息：加权求和得到输出  

利用计算得到的注意力权重 $\alpha _ { i j }$ 对值向量 $\nu _ { _ j }$ 进行加权求和，从而生成新的输出向量：  

$$
\mathrm { A t t e n t i o n } ( q _ { i } , K , V ) = \sum _ { j = 1 } ^ { n } \alpha _ { i j } \nu _ { j }
$$  

这样，每个输出向量就包含了来自所有输入位置的相关信息，其贡献程度由对应的$\alpha _ { i j }$ 决定。  

![](images/523bda0f34a1a75a95b932a4b06b001c788bb6fccd75cd5c4156b5fbe46d825e.jpg)  
图 3.6 归一化与聚合信息  

五、多头注意力机制  

为了使模型能够从不同的子空间中捕捉多种语义信息，引入了多头注意力。具体做法是：  

1. 将输入分别映射为多个不同的 Q、K 和 $\mathsf { v }$ （每一组称为一个“头”），例如将嵌入维度 $d _ { \mathrm { m o d e l } }$ 分成 $h$ 个子空间，每个头的维度为 $d _ { \mathrm { h e a d } } = d _ { \mathrm { m o d e l } } / h$ 。  

2. 对每个头分别执行上述注意力计算流程，得到各自的输出。  

3. 将所有头的输出拼接起来，再通过一个线性变换得到最终输出：MultiHead $( \boldsymbol { \mathcal { Q } } , \boldsymbol { K } , \boldsymbol { V } ) = \mathrm { C o n c a t } ( \mathrm { h e a d } _ { 1 } , . . . , \mathrm { h e a d } _ { h } )$ 其中 $\boldsymbol { W } ^ { o }$ 是最终的输出投影矩阵。  

# 六、Masking 掩码  

在解码器中，为了防止未来信息泄露（即当前生成的词不应依赖于未来词），通常会对注意力得分矩阵应用“因果掩码”（causal mask），将未来位置的得分设为负无穷，从而在 softmax 后使其权重为 0。  

# C. 网络结构  

# 一、编码器（Encoder）：  

整合智能体、任务的信息，确立智能体、任务之间的相关性。编码器部分分为任务编码器、智能体编码器以及跨模态（交叉）编码器。  

任务编码器：  

1. 将任务状态通过一个线性投影层映射到 d 维嵌入空间。  

2. 利用多头自注意力层（Multi-Head Self-Attention, MHA）对任务嵌入进行编码，从而捕捉任务之间的空间依赖和相互影响，同时计算出全局任务“全景” $\overline { { h _ { { } _ { T } } } }$ （通过对所有任务嵌入取平均）。  

智能体编码器：  

类似地，将所有智能体的状态 $N _ { t } ^ { i }$ ​ 经过线性投影后，再利用多头自注意力层得到编码表示，并提取全局智能体信息 $\overline { { h } } _ { \scriptscriptstyle N }$ 。  

交叉编码器（Cross Encoder）：  

通过两个多头注意力层实现信息交互：  

1. 利用智能体编码器的输出作为查询、任务编码器输出作为键和值，得到“agent–task”上下文 $h _ { N T } ^ { \prime }$ 。  

2. 反过来得到“task–agent”上下文 hTN 。  

# 二、解码器（Decoder）：  

将编码器的输出再次整合起来，形成智能体当前的状态，并输出任务的选择概率，  

其流程如下：  

1. 特征提取：针对当前决策智能体，从“agent–task”上下文 $h _ { N T } ^ { \prime }$ 的对应行提取出个体特征，并与全局的智能体和任务信息 $( \overline { { h } } _ { N } , \overline { { h } } _ { T } )$ 拼接，通过一个 MLP 得到当前状态表示 h i 。  

2. 注意力增强：将 $h _ { a i }$ ​ 作为查询输入到另一个多头注意力模块中，并结合“task–agent”上下文 $h _ { T N } ^ { \prime }$ （同时利用决策掩码 $M _ { i }$ 过滤无效任务），以获得增强的表示 hai 。  

3. 策略输出：最终利用注意力得分在任务嵌入上计算出概率分布，作为当前智能体对各任务的选择策略。  

![](images/0d4e37907a7b0dd9d922a7def5a846075dd80bbada096ccf8e180c1c86599740.jpg)  
图 3.7 网络结构  

# D 智能方法验证与对比  

用基于Attention 机制的神经网络应用于本场景，能够有效解决单任务无人机（ST）与多无人机任务（MR）间的长期分配（TA）问题，且在于传统方法的对比中消耗的步数更少，验证了其有效性与优越性。  

![](images/70a3871d6bf7773f8af02568499226f9ed693b740a61c25a1d1e12e06769fd83.jpg)  
图 3.8 在同一场景下的测试，智能方法完成更快  

![](images/7eeeb7176df1de8f9f4558d232aa3720f35ee8a8e6b9ef1dae2e0b91a27c69ec.jpg)  
图 3.9 运行 50 批次不同场景后的步数对比图(越少越好)  

# 3.2.4 用神经网络解决异构无人机集群任务分配问题  

修改场景建模，将单一无人机转变为属于不同物种集 $S = \{ s _ { 1 } , s _ { 2 } , . . . , s _ { k _ { s } } \}$ 的无人机，每个 物 种 集 具 有 不 同 的 能 力 $\boldsymbol { C } _ { a _ { j } } = [ c _ { 1 } ^ { j } , c _ { 2 } ^ { j } , . . . , c _ { k _ { s } } ^ { j } ]$ ， 每 个 任 务 也 有 不 同 的 任 务 需 求$\boldsymbol { q } _ { m _ { i } } = [ \boldsymbol { q } _ { 1 } ^ { j } , \boldsymbol { q } _ { 2 } ^ { j } , . . . , \boldsymbol { q } _ { k _ { s } } ^ { j } ]$ 。将任务分配扩展到了异构智能体的任务分配上。  

经过训练后的Attention 机制的神经网络，能解决该异构智能体的任务分配问题，而传统方法需要额外的任务分解等算法辅助才能解决该任务分配问题。  

![](images/93470ee2348352b69c3aa7fa87924af76338d9d157f15a83c3b9ebd33fd93a8d.jpg)  
图 3.10 异构任务分配场景  

# 3.2.5 用神经网络解决任务数量变化的动态任务分配问题  

当任务数量变化时，神经网络需要处理维度不同的输入，基本的线性层无法解决。而像大语言模型这类模型能够处理长短不一的输入的原因，是因为其运用了填充（padding）与自注意力(attention)机制[14]。本研究沿用了这一框架，通过填充，将不同需求的任务统一填充至同一维度的向量，并通过取消上下文数量限制，将不同数量的任务统一输入attention 层，生成统一的状态表示，最终经过解码器输出策略。  

![](images/6b9df7b377903300cb5010f313519d89f7e9b3a5db6beeb8aaa3c9a9f19b5e2f.jpg)  
图 3.11 填充（padding）操作与 Attention 机制处理不同长短的输入  

![](images/83a09226892c6a3c0b88fada9bda6931eb4718ab9bd28b135c5eae8a6f254f0b.jpg)  
图 3.12 动态环境适应  

本研究处理任务数量变化的动态环境的流程如下：  

1. 网络根据初始任务输入，自主迭代计算出无人机集群的完整分配方案；2. 在分配方案执行过程中，若出现任务数量增加，则在此刻暂停，将新任务与旧任务整合，输入网络进行重新分配后得到新的分配方法；3. 选择新分配方法包含新任务的无人机，更新其旧分配方法；4. 无人机执行新的分配方案。  

# 4 尚存问题及后期工作安排  

# 4.1 尚存问题  

# 4.1.1 动态环境适应问题  

目前尚未解决无人机失效的动态环境问题，且需优化分配方案更新策略。  

解决方案是：  

1. 设置无人机掩码，让网络不对已失效的无人机进行任务分配，同时重新训练网络，使其能理解无人机失效情况。  

2. 设计新的训练机制，随机生成动态情况，提高网络的动态适应能力。  

3. 增加环境更新时的任务状态检测与异常处理机制，防止分配方案差别过大导致的场景崩溃问题。  

# 4.1.2 算法对比与实机验证  

目前的对比方法与数据较少，无法完全体现优越性，且无实机验证。  

解决方案是：  

1. 将蚁群算法、整数规划方法、MAPPO 算法应用到本场景中，并进行多维度对比（完成时间、成功率、行进距离、计算时间）。2. 与学校或研究所合作，使用无人机集群平台进行实机验证。  

# 4.2 后期工作安排  

![](images/0347bd930db18be9ab596768be029e935054b82bf6a64908a5c4bab3948fa510.jpg)  
图 4.1 后期工作安排  

# 参考文献  

[1] JAVED S, HASSAN A, AHMAD R, et al. State-of-the-Art and Future Research Challenges in UAV Swarms[J]. Ieee Internet of Things Journal, 2024, 11(11): 19023-45.   
[2] LI Q, XIONG H, DING Y, et al. A Review of Unmanned Aerial Vehicle Swarm Task Assignment[M]. Advances in Guidance, Navigation and Control. 2023: 6469-79.   
[3] DAI W, BIDWAI A, SARTORETTI G. Dynamic Coalition Formation and Routing for Multirobot Task Allocation via Reinforcement Learning, F 2024]. IEEE.   
[4] GERKEY B P, MATARIC M J. A formal analysis and taxonomy of task allocation in multi-robot systems[J]. International Journal of Robotics Research, 2004, 23(9): 939-54.   
[5] DUAN X, LIU H, TANG H, et al. A Novel Hybrid Auction Algorithm for Multi-UAVs Dynamic Task Assignment[J]. IEEE Access, 2020, 8: 86207-22.   
[6] HUNT S, MENG Q, HINDE C, et al. A Consensus-Based Grouping Algorithm for Multi-agent Cooperative Task Allocation with Complex Requirements[J]. Cognitive Computation, 2014, 6(3): 338-50.   
[7] QI N, HUANG Z Q, SUN W, et al. Coalitional Formation-Based Group-Buying for UAV-Enabled Data Collection: An Auction Game Approach[J]. Ieee Transactions on Mobile Computing, 2023, 22(12): 7420-37.   
[8] ISMAIL S, SUN L, IEEE. Decentralized Hungarian-Based Approach for Fast and Scalable Task Allocation; proceedings of the International Conference on Unmanned Aircraft Systems (ICUAS), Miami, FL, F Jun 13-16, 2017[C]. 2017.   
[9] AZOULAY R, HADDAD Y, RECHES S. Machine Learning Methods for UAV Flocks Management-A Survey[J]. Ieee Access, 2021, 9: 139146-75.   
[10] LIU D, DOU L Q, ZHANG R L, et al. Multi-Agent Reinforcement Learning-Based Coordinated Dynamic Task Allocation for Heterogenous UAVs[J]. Ieee Transactions on Vehicular Technology, 2023, 72(4): 4372-83.   
[11] XIAO K, LU J, NIE Y, et al. A Benchmark for Multi-UAV Task Assignment of an Extended Team Orienteering Problem[J]. 2020.   
[12] MORDATCH I, ABBEEL P. Emergence of grounded compositional language in multi-agent populations[Z]. Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence. New Orleans, Louisiana, USA; AAAI Press. 2018: Article 183   
[13] DAI W H, RAI U, CHIUN J, et al. Heterogeneous Multi-robot Task Allocation and Scheduling via Reinforcement Learning[J]. Ieee Robotics and Automation Letters, 2025, 10(3): 2654-61.   
[14] VASWANI A, SHAZEER N, PARMAR N, et al. Attention is all you need[Z]. Proceedings of the 31st International Conference on Neural Information Processing Systems. Long Beach, California, USA; Curran Associates Inc. 2017: 6000–10  